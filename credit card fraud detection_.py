#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Converted from Jupyter Notebook: notebook.ipynb
Conversion Date: 2025-10-23T16:06:46.110Z
"""

# # **Credit Card Fraud Detection Project** üí≥‚ö†Ô∏è
# 
# Ce projet **vise √† d√©tecter les transactions frauduleuses** sur des cartes de cr√©dit en utilisant des techniques de **Data Science et Machine Learning**.  
# 
# Le dataset utilis√© contient les colonnes suivantes :  
# 
# - **Time** ‚è∞ : Le temps √©coul√© (en secondes) depuis la premi√®re transaction dans le dataset.  
# - **V1, V2, ..., V28** üî¢ : Variables anonymis√©es issues d'une **transformation PCA** pour prot√©ger la confidentialit√© des clients.  
# - **Amount** üí∞ : Montant de la transaction.  
# - **Class** üö® : Indique si la transaction est **frauduleuse (1)** ou **non frauduleuse (0)**.  
# 
# ## **Objectifs du projet** üéØ
# 1. **Analyser le dataset** pour comprendre la distribution des transactions et des fraudes.  
# 2. **Pr√©traiter les donn√©es** (normalisation, gestion du d√©s√©quilibre de classes, etc.).  
# 3. **Entra√Æner des mod√®les de machine learning** pour d√©tecter les fraudes.  
# 4. **√âvaluer les performances** des mod√®les avec des m√©triques adapt√©es comme la **precision, recall et F1-score**.  
# 5. Fournir une **solution pr√©dictive efficace** pour aider les institutions financi√®res √† **r√©duire les pertes li√©es on des fraudes**.
# pr√©cision üéØ**  learn, XGBoost üèéÔ∏è  
# 


# # **Imports et Pr√©paration de l'Environnement** üõ†Ô∏è
# premi√®rement on va oc **imporrte toutes les biblioth√®ques et modules n√©cessaires** pour le projet de d√©tection de fraude sur les cartes de cr√©dit. Il pr√©pare l'environnement pour :
# 
# - **Manipulation et analyse des donn√©es** : `pandas`, `numpy`  
# - **Visualisation** : `matplotlib`, `seaborn`  
# - **Statistiques et transformations** : `scipy`, `stats`, `boxcox`  
# - **Pr√©traitement des donn√©es** : `sklearn.preprocessing`, `StandardScaler`  
# - **Mod√©lisation et apprentissage automatique** :  
#   - R√©gression et classification : `LogisticRegression`, `Ridge`, `Lasso`  
#   - Arbres et ensembles : `DecisionTreeClassifier`, `RandomForestClassifier`, `AdaBoostClassifier`  
#   - Support Vector Machines et KNN : `SVC`, `KNeighborsClassifier`  
#   - XGBoost : `XGBClassifier`, `plot_importance`  
# - **Validation et recherche d'hyperparam√®tres** : `train_test_split`, `KFold`, `StratifiedKFold`, `GridSearchCV`, `RandomizedSearchCV`, `RepeatedKFold`  
# - **Gestion du d√©s√©quilibre des classes** : `RandomOverSampler`, `imblearn.over_sampling`  
# - **Autres outils utiles** : gestion du temps (`time`), suppression des warnings tion de fraudes.
# 


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn import metrics
from sklearn import linear_model
from sklearn.svm import SVC
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import Ridge, Lasso, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegressionCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RepeatedKFold
from imblearn.over_sampling import RandomOverSampler
from imblearn import over_sampling
import sklearn
import time
import seaborn as sns
from scipy import stats
from scipy.stats import norm, skew
from scipy.stats import boxcox_normmax
from scipy.special import boxcox1p
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance

import warnings 
warnings.filterwarnings("ignore")

import mlflow
import mlflow.sklearn
from datetime import datetime

mlflow.set_tracking_uri("http://127.0.0.1:5000") 
mlflow.set_experiment("CreditCard_Fraud_Detection")


# 
# D'abord on va **charger le dataset des transactions par carte de cr√©dit** depuis le fichier `creditcard.csv` dans un **DataFrame pandas** nomm√© `credit_card_transactions_data`.  
# 
# Le DataFrame contiendra toutes les colonnes du dataset, y compris les **variables anonymis√©es (V1 √† V28)**, le **montant de la transaction (Amount)** et la **classe indiquant la fraude (Class)**.  
# 
# Cela constitue la **base pour toutes les √©tapes suivantes** du projet : exploration, pr√©traitement, mod√©lisation et √©valuation des mod√®les.
# 


credit_card_transactions_data = pd.read_csv("creditcard.csv")

credit_card_transactions_data.head()

# Cette ligne affiche un **aper√ßu des informations du dataset** `credit_card_transactions_data`.  
# 
# Elle permet de conna√Ætre :  
# - Le **nombre total de transactions** : 284‚ÄØ807.  
# - Le **nombre et le type de colonnes** : 31 colonnes au total, dont 30 **float64** (Time, V1 √† V28, Amount) et 1 **int64** (C).s ).  
# - La **pr√©sence de valeurs manquantes** : aucolonne ne contient de valeurs nulles


credit_card_transactions_data.info()

credit_card_transactions_data.isnull().sum()

# Cette ligne permet de **comprendre la distribution des classes** dans le dataset `credit_card_transactions_data`.  
# 
# - **0 (non frauduleuse)** : 284‚ÄØ315 transactions  
# - **1 (frauduleuse)** : 492 transactions  
# 
# On remarque que le dataset est **tr√®s d√©s√©quilibr√©**, avec une proportion tr√®s faible de fraudeaude.
# 


credit_card_transactions_data['Class'].value_counts()

# Cette commande calcule et **visualise la proportion de chaque classe** dans le dataset.  
# 
# - Elle montre que les transactions **non frauduleuses (Class 0)** repr√©sentent environ **99,83‚ÄØ%** des donn√©es.  
# - Les transactions **frauduleuses (Class 1)** repr√©sentent seulement **0,17‚ÄØ%** des donn√©es.  
# 
# La commande `.plot.pie()` cr√©e un **diagramme circulaire** pour visualiser ce d√©s√©quilibre, ce qui aide √† **comprendre l'importance de traiter le probl√®me des classes d√©s√©quilibr√©es** avant de former un mod√®le.
# 


print((credit_card_transactions_data.groupby("Class")["Class"].count() / credit_card_transactions_data["Class"].count()) * 100)
((credit_card_transactions_data.groupby("Class")["Class"].count() / credit_card_transactions_data["Class"].count()) * 100).plot.pie()

# Ce bloc cr√©e un **graphique pour visualiser le nombre de transactions par classe**.  
# 
# - `sns.countplot` affiche le **nombre de transactions non frauduleuses (Class 0)** et **frauduleuses (Class 1)**.  
# - Les axes sont √©tiquet√©s pour montrer le **nombre d'enregistrements par classe**.  
# - Le titre met en √©vidence que le graphique repr√©sente le **comptage des classes**.  
# 


plt.figure(figsize=(7,5))
sns.countplot(x='Class', data=credit_card_transactions_data)
plt.title("Class count" , fontsize=18)
plt.xlabel("Record count by class", fontsize=15)
plt.ylabel("Count", fontsize=15)
plt.show()

# Ce bloc calcule et **visualise la matrice de corr√©lation** du dataset .  
# 
# - `credit_card_transactions_data.corr()` calcule la **corr√©lation entre toutes les colonnes num√©riques**.  
# - `sns.heatmap` cr√©e une **carte de chaleur** pour repr√©senter visuellement ces corr√©lations, avec les valeurs annot√©es pour plus de clart√©.  
# - La palette `coolwarm` permet de distinguer facilement les **corr√©lations positives et n√©gatives**.  
# 
# Cette visualisation aide √† **identifier les relations entre les variables** et peut guider la **s√©lection des caract√©ristiques** ou la **d√©tection de variables fortement corr√©l√©es** avant la mod√©lisation.
# 


corr = credit_card_transactions_data.corr()
corr

plt.figure(figsize=(24,18))

sns.heatmap(corr, cmap="coolwarm", annot=True)
plt.show()

# Ce bloc transforme la colonne **`Time`** du dataset en plusieurs composantes temporelles pour faciliter l'analyse :  
# 
# - `pd.to_timedelta` convertit la colonne `Time` (en secondes) en **objet timedelta**.  
# - De cette transformation, on extrait :  
#   - **`Time_Day`** : le nombre de jours √©coul√©s depuis la premi√®re transaction.  
#   - **`Time_Hour`** : l'heure de la transaction dans la journ√©e.  
#   - **`Time_Min`** : les minutes de la transaction.  
# - Ensuite, certaines colonnes sont **supprim√©es** (`Time`, `Time_Day`, `Time_Min`) pour ne conserver que les informations pertinentes pour la mod√©lisation, comme `Time_Hour`.  
# 


Delta_time = pd.to_timedelta(credit_card_transactions_data['Time'], unit='s')

credit_card_transactions_data['Time_Day'] = (Delta_time.dt.components.days).astype(int)
credit_card_transactions_data['Time_Hour'] = (Delta_time.dt.components.hours).astype(int)
credit_card_transactions_data['Time_Min'] = (Delta_time.dt.components.minutes).astype(int)

credit_card_transactions_data.drop(['Time', 'Time_Day', 'Time_Min'], axis=1, inplace=True)


# Ce bloc pr√©pare les **features et la cible** pour la mod√©lisation :  
# 
# - `X` contient toutes les colonnes **sauf `Class`**, repr√©sentant les **caract√©ristiques**.  
# - `y` contient uniquement la colonne **`Class`**, qui est la **variable cible** indiquant si une transaction est **frauduleuse (1) ou non (0)**.  
# 
# Cette s√©paration est essentielle pour **entra√Æner et √©valuer les mod√®les de machine learning**.
# 


X = credit_card_transactions_data.drop(["Class"], axis=1)
y = credit_card_transactions_data["Class"]

X.tail()

# Cette ligne **s√©pare le dataset en ensembles d'entra√Ænement et de test** :  
# 
# - `X_train` et `y_train` : **donn√©es d'entra√Ænement**, utilis√©es pour **entra√Æner le mod√®le**.  
# - `X_test` et `y_test` : **donn√©es de test**, utilis√©es pour **√©valuer les performances** du mod√®le sur des donn√©es non vues.  
# - `test_size=0.2` signifie que **20‚ÄØ% des donn√©es** sont r√©serv√©es pour le test.  
# - `random_state=100` garantit que la **s√©paration est reproductible**.
# 


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

# Ce bloc cr√©e des **histogrammes superpos√©s pour chaque feature** afin de comparer les distributions entre les transactions **l√©gitimes** et **frauduleuses** :  
# 
# - `cols` contient la liste de toutes les colonnes/features.  
# - `legit_records` et `fraud_records` sont des **masques bool√©ens** pour s√©parer les transactions non frauduleuses et frauduleuses.  
# - Pour chaque colonne, `sns.distplot` trace :  
#   - En **vert** : la distribution des transactions l√©gitimes.  
#   - En **rouge** : la distribution des transactions frauduleuses.  
# - Les sous-graphes (`plt.subplot`) permettent de **visualiser toutes les features dans une seule figure**.  
# 
# Cette visualisation aide √† **identifier quelles variables peuvent mieux diff√©rencier les fraudes des transactions normales**.
# 


cols = list(X.columns.values)

legit_records = credit_card_transactions_data.Class == 0
fraud_records = credit_card_transactions_data.Class == 1

plt.figure(figsize=(20, 60))
for n, col in enumerate(cols):
    plt.subplot(10,3,n+1)
    sns.distplot(X[col][legit_records], color='green')
    sns.distplot(X[col][fraud_records], color='red')
    plt.title(col, fontsize=17)

plt.show()
    

# Cette ligne cr√©e un **DataFrame vide nomm√© `df_Results`** pour **stocker les r√©sultats des diff√©rents mod√®les** test√©s dans le projet.  
# 
# - Les colonnes du DataFrame sont :  
#   - **`Methodology`** : la m√©thode ou approche utilis√©e.  
#   - **`Model`** : le nom du mod√®le de machine learning.  
#   - **`Accuracy`** : la pr√©cision obtenue sur les donn√©es de test.  
#   - **`roc_value`** : la valeur de l'AUC-ROC pour √©valuer la capacit√© du mod√®le √† distinguer les classes.  
#   - **`threshold`** : le seuil choisi pour classer une transaction comme frauduleuse ou non.  
# 
# Ce DataFrame servira √† **comparer facilement les performances de tous les mod√®les exp√©riment√©s**.
# 


df_Results = pd.DataFrame(columns=['Methodology', 'Model', 'Accuracy', 'roc_value', 'threshold'])

# Cette fonction **`buildAndRunLogisticModels`** permet de **construire, entra√Æner et √©valuer des mod√®les de r√©gression logistique** avec r√©gularisation L1 et L2 sur le dataset de transactions :  
# 
# - **Hyperparam√®tres et validation crois√©e** :  
#   - `num_C` d√©finit une s√©rie de valeurs pour le param√®tre de r√©gularisation C.  
#   - `KFold` est utilis√© pour la **validation crois√©e √† 10 plis**.  
# - **Mod√®les** :  
#   - `LogisticRegressionCV` avec **L1** (lasso) et **L2** (ridge) pour g√©rer la r√©gularisation.  
# - **Entra√Ænement** : les deux mod√®les sont **ajust√©s sur les donn√©es d'entra√Ænement**.  
# - **√âvaluation** :  
#   - Calcul de **l‚Äôaccuracy**, **matrices de confusion**, **classification report** et **valeurs ROC-AUC**.  
#   - Calcul du **seuil optimal** pour classifier une transaction comme frauduleuse.  
#   - Trac√© des **courbes ROC** pour visualiser la performance sur les donn√©es de test.  
# - **Stockage des r√©sultats** :  
#   - Les performances de chaque mod√®le (Accuracy, ROC-AUC, seuil) sont ajout√©es au **DataFrame `df_Results`** pour comparaison ult√©rieure.  
# 
# Cette fonction permet donc de **tester et comparer efficacement deux approches de r√©gression logistique** sur des donn√©es d√©s√©quilibr√©es.
# 


def buildAndRunLogisticModels(df_Results, Methodology, X_train, y_train, X_test, y_test):

    num_C = list(np.power(10.0, np.arange(-10, 10)))
    cv_num = KFold(n_splits=10, shuffle=True, random_state=42)

    searchCV_l2 = LogisticRegressionCV(
        Cs=num_C, penalty='l2', scoring='roc_auc', cv=cv_num,
        random_state=42, max_iter=10000, fit_intercept=True,
        solver='newton-cg', tol=10
    )

    searchCV_l1 = LogisticRegressionCV(
        Cs=num_C, penalty='l1', scoring='roc_auc', cv=cv_num,
        random_state=42, max_iter=10000, fit_intercept=True,
        solver='liblinear', tol=10
    )

    # ==================== MLflow RUN pour L2 ====================
    with mlflow.start_run(run_name=f"Logistic_L2_{Methodology}_{datetime.now().strftime('%H%M%S')}"):
        searchCV_l2.fit(X_train, y_train)
        y_pred_l2 = searchCV_l2.predict(X_test)
        y_pred_probs_l2 = searchCV_l2.predict_proba(X_test)[:, 1]

        Accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, y_true=y_test)
        roc_value_l2 = metrics.roc_auc_score(y_test, y_pred_probs_l2)
        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_probs_l2)
        threshold_l2 = thresholds[np.argmax(tpr - fpr)]

        mlflow.log_param("Model", "Logistic Regression")
        mlflow.log_param("Regularization", "L2")
        mlflow.log_param("Methodology", Methodology)
        mlflow.log_param("Best_C", searchCV_l2.C_[0])
        mlflow.log_metric("Accuracy", Accuracy_l2)
        mlflow.log_metric("ROC_AUC", roc_value_l2)
        mlflow.log_metric("Threshold", threshold_l2)

        mlflow.sklearn.log_model(searchCV_l2, artifact_path="model")

        df_Results = pd.concat([df_Results, pd.DataFrame({
            'Methodology': [Methodology],
            'Model': ['Logistic Regression with L2 Regularization'],
            'Accuracy': [Accuracy_l2],
            'roc_value': [roc_value_l2],
            'threshold': [threshold_l2]
        })], ignore_index=True)

    # ==================== MLflow RUN pour L1 ====================
    with mlflow.start_run(run_name=f"Logistic_L1_{Methodology}_{datetime.now().strftime('%H%M%S')}"):
        searchCV_l1.fit(X_train, y_train)
        y_pred_l1 = searchCV_l1.predict(X_test)
        y_pred_probs_l1 = searchCV_l1.predict_proba(X_test)[:, 1]

        Accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, y_true=y_test)
        roc_value_l1 = metrics.roc_auc_score(y_test, y_pred_probs_l1)
        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_probs_l1)
        threshold_l1 = thresholds[np.argmax(tpr - fpr)]

        # Log param√®tres et m√©triques
        mlflow.log_param("Model", "Logistic Regression")
        mlflow.log_param("Regularization", "L1")
        mlflow.log_param("Methodology", Methodology)
        mlflow.log_param("Best_C", searchCV_l1.C_[0])
        mlflow.log_metric("Accuracy", Accuracy_l1)
        mlflow.log_metric("ROC_AUC", roc_value_l1)
        mlflow.log_metric("Threshold", threshold_l1)

        # Sauvegarde du mod√®le
        mlflow.sklearn.log_model(searchCV_l1, artifact_path="model")

        df_Results = pd.concat([df_Results, pd.DataFrame({
            'Methodology': [Methodology],
            'Model': ['Logistic Regression with L1 Regularization'],
            'Accuracy': [Accuracy_l1],
            'roc_value': [roc_value_l1],
            'threshold': [threshold_l1]
        })], ignore_index=True)

    return df_Results


# Cette fonction **`buildAndRunKNNModels`** permet de **construire, entra√Æner et √©valuer un mod√®le K-Nearest Neighbors (KNN)** sur le dataset de transactions :  
# 
# - **Mod√®le KNN** :  
#   - `KNeighborsClassifier` avec **5 voisins** et parall√©lisation (`n_jobs=16`) pour acc√©l√©rer l'entra√Ænement.  
# - **Entra√Ænement** : le mod√®le est ajust√© sur les **donn√©es d'entra√Ænement** (`X_train`, `y_train`).  
# - **√âvaluation** :  
#   - Calcul de **l‚Äôaccuracy**, **matrice de confusion** et **classification report** sur l'ensemble de test.  
#   - Calcul des **probabilit√©s pr√©dites** pour la classe positive (fraude).  
#   - Calcul de la **valeur ROC-AUC** et du **seuil optimal** pour classifier une transaction comme frauduleuse.  
#   - Trac√© de la **courbe ROC** pour visualiser la performance du mod√®le.  
# - **Stockage des r√©sultats** : les performances du mod√®le (Accuracy, ROC-AUC, seuil) sont ajout√©es au **DataFrame `df_Results`** pour comparaison avec d'autres mod√®les.  
# 
# Cette fonction fournit une **√©valuation compl√®te du KNN** dans le contexte de la d√©tection de fraude.
# 


def buildAndRunKNNModels(df_Results, Methodology, X_train, y_train, X_test, y_test):

    # ==================== D√©marrage du tracking MLflow ====================
    with mlflow.start_run(run_name=f"KNN_{Methodology}_{datetime.now().strftime('%H%M%S')}"):
        
        # --- Entra√Ænement du mod√®le ---
        knn = KNeighborsClassifier(n_neighbors=5, n_jobs=16)
        knn.fit(X_train, y_train)
        score = knn.score(X_test, y_test)
        print("Model score :", score)

        # --- Pr√©dictions et √©valuation ---
        y_pred = knn.predict(X_test)
        KNN_Accuracy = metrics.accuracy_score(y_pred=y_pred, y_true=y_test)
        print("Confusion Matrix:", metrics.confusion_matrix(y_test, y_pred))
        print("Classification Report:", metrics.classification_report(y_test, y_pred))

        knn_probs = knn.predict_proba(X_test)[:, 1]
        knn_roc_value = metrics.roc_auc_score(y_test, knn_probs)
        fpr, tpr, thresholds = metrics.roc_curve(y_test, knn_probs)
        threshold = thresholds[np.argmax(tpr - fpr)]
        roc_auc = metrics.auc(fpr, tpr)

        print(f"KNN ROC value: {knn_roc_value:.4f}")
        print(f"Optimal threshold: {threshold:.4f}")
        plt.plot(fpr, tpr, label="Test, auc=" + str(round(roc_auc, 4)))
        plt.legend(loc=4)
        plt.show()

        # --- Log des param√®tres ---
        mlflow.log_param("Model", "KNN")
        mlflow.log_param("Methodology", Methodology)
        mlflow.log_param("n_neighbors", 5)
        mlflow.log_param("n_jobs", 16)

        # --- Log des m√©triques ---
        mlflow.log_metric("Accuracy", KNN_Accuracy)
        mlflow.log_metric("ROC_AUC", knn_roc_value)
        mlflow.log_metric("Threshold", threshold)

        # --- Enregistrement du mod√®le dans MLflow ---
        mlflow.sklearn.log_model(knn, artifact_path="model")

        # --- Enregistrement des r√©sultats dans le DataFrame ---
        df_Results = pd.concat([
            df_Results,
            pd.DataFrame({
                'Methodology': [Methodology],
                'Model': ['KNN'],
                'Accuracy': [KNN_Accuracy],
                'roc_value': [knn_roc_value],
                'threshold': [threshold]
            })
        ], ignore_index=True)

    return df_Results


# Cette fonction **`buildAndRunTreeModels`** permet de **construire, entra√Æner et √©valuer des mod√®les d'arbre de d√©cision** sur le dataset de transactions :  
# 
# - **Crit√®res d'arbre** : `gini` et `entropy` sont test√©s pour mesurer la **puret√© des n≈ìuds** lors de la construction de l'arbre.  
# - **Entra√Ænement** : chaque arbre est ajust√© sur les **donn√©es d'entra√Ænement** (`X_train`, `y_train`).  
# - **√âvaluation** :  
#   - Calcul de **l‚Äôaccuracy**, **matrice de confusion** et **classification report** sur l'ensemble de test.  
#   - Calcul des **probabilit√©s pr√©dites** pour la classe positive (fraude).  
#   - Calcul de la **valeur ROC-AUC** et du **seuil optimal** pour classifier une transaction comme frauduleuse.  
#   - Trac√© de la **courbe ROC** pour visualiser la performance du mod√®le pour chaque crit√®re.  
# - **Stockage des r√©sultats** : les performances de chaque arbre (Accuracy, ROC-AUC, seuil) sont ajout√©es au **DataFrame `df_Results`** pour comparaison avec d'autres mod√®les.  
# 
# Cette fonction permet ainsi de **comparer facilement l'impact du crit√®re choisi sur les performances de l'arbre de d√©cision**.
# 


def buildAndRunTreeModels(df_Results, Methodology, X_train, y_train, X_test, y_test):
    criteria = ['gini', 'entropy']

    for c in criteria:
        with mlflow.start_run(run_name=f"DecisionTree_{c}_{Methodology}_{datetime.now().strftime('%H%M%S')}"):
            # --- Entra√Ænement du mod√®le ---
            dt = DecisionTreeClassifier(criterion=c, random_state=42)
            dt.fit(X_train, y_train)
            y_pred = dt.predict(X_test)
            test_score = dt.score(X_test, y_test)
            tree_probs = dt.predict_proba(X_test)[:, 1]
            tree_roc_value = metrics.roc_auc_score(y_test, tree_probs)
            fpr, tpr, thresholds = metrics.roc_curve(y_test, tree_probs)
            threshold = thresholds[np.argmax(tpr - fpr)]
            roc_auc = metrics.auc(fpr, tpr)

            # --- Affichage des r√©sultats ---
            print(f"{c} score : {test_score:.4f}")
            print("Confusion Matrix:", metrics.confusion_matrix(y_test, y_pred))
            print("Classification Report:", metrics.classification_report(y_test, y_pred))
            print(f"ROC AUC: {tree_roc_value:.4f} | Threshold: {threshold:.4f}")
            plt.plot(fpr, tpr, label=f"Test ({c}), auc={roc_auc:.4f}")
            plt.legend(loc=4)
            plt.show()

            # --- Log dans MLflow ---
            mlflow.log_param("Model", "Decision Tree")
            mlflow.log_param("Criterion", c)
            mlflow.log_param("Methodology", Methodology)
            mlflow.log_metric("Accuracy", test_score)
            mlflow.log_metric("ROC_AUC", tree_roc_value)
            mlflow.log_metric("Threshold", threshold)

            # Sauvegarde du mod√®le
            mlflow.sklearn.log_model(dt, artifact_path="model")

            # Enregistrement du r√©sultat dans ton DataFrame
            df_Results = pd.concat([
                df_Results,
                pd.DataFrame({
                    'Methodology': [Methodology],
                    'Model': [f'Decision Tree ({c})'],
                    'Accuracy': [test_score],
                    'roc_value': [tree_roc_value],
                    'threshold': [threshold]
                })
            ], ignore_index=True)

    return df_Results


# Cette fonction **`buildAndRunRandomForestModels`** permet de **construire, entra√Æner et √©valuer un mod√®le Random Forest** sur le dataset de transactions :  
# 
# - **Mod√®le Random Forest** :  
#   - `RandomForestClassifier` avec **100 arbres**, bootstrap activ√© et s√©lection al√©atoire des features (`max_features='sqrt'`).  
# - **Entra√Ænement** : le mod√®le est ajust√© sur les **donn√©es d'entra√Ænement** (`X_train`, `y_train`).  
# - **√âvaluation** :  
#   - Calcul de **l‚Äôaccuracy**, **matrice de confusion** et **classification report** sur l'ensemble de test.  
#   - Calcul des **probabilit√©s pr√©dites** pour la classe positive (fraude).  
#   - Calcul de la **valeur ROC-AUC** et du **seuil optimal** pour classifier une transaction comme frauduleuse.  
#   - Trac√© de la **courbe ROC** pour visualiser la performance du mod√®le.  
# - **Stockage des r√©sultats** : les performances du mod√®le (Accuracy, ROC-AUC, seuil) sont ajout√©es au **DataFrame `df_Results`** pour comparaison avec d'autres mod√®les.  
# 
# Cette fonction fournit une **√©valuation compl√®te du Random Forest** dans le contexte de la d√©tection de fraude.
# 


def buildAndRunRandomForestModels(df_Results, Methodology, X_train, y_train, X_test, y_test):

    with mlflow.start_run(run_name=f"RandomForest_{Methodology}_{datetime.now().strftime('%H%M%S')}"):
        # --- Entra√Ænement du mod√®le ---
        RF_model = RandomForestClassifier(
            n_estimators=100,
            bootstrap=True,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )

        RF_model.fit(X_train, y_train)
        RF_test_score = RF_model.score(X_test, y_test)
        RF_predictions = RF_model.predict(X_test)

        # --- √âvaluation ---
        print(f"Model Accuracy : {RF_test_score:.4f}")
        print("Confusion Matrix :\n", metrics.confusion_matrix(y_test, RF_predictions))
        print("Classification Report :\n", metrics.classification_report(y_test, RF_predictions))

        rf_probs = RF_model.predict_proba(X_test)[:, 1]
        rf_roc_value = metrics.roc_auc_score(y_test, rf_probs)
        fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_probs)
        threshold = thresholds[np.argmax(tpr - fpr)]
        roc_auc = metrics.auc(fpr, tpr)

        print(f"RF ROC AUC: {rf_roc_value:.4f}")
        print(f"RF Threshold: {threshold:.4f}")
        print("ROC for test dataset:", '{:.1%}'.format(roc_auc))

        plt.plot(fpr, tpr, label=f"Test, AUC={roc_auc:.4f}")
        plt.legend(loc=4)
        plt.show()

        # --- Log dans MLflow ---
        mlflow.log_param("Model", "Random Forest")
        mlflow.log_param("Methodology", Methodology)
        mlflow.log_param("n_estimators", 100)
        mlflow.log_param("max_features", "sqrt")
        mlflow.log_metric("Accuracy", RF_test_score)
        mlflow.log_metric("ROC_AUC", rf_roc_value)
        mlflow.log_metric("Threshold", threshold)

        # Sauvegarde du mod√®le
        mlflow.sklearn.log_model(RF_model, artifact_path="model")

        # --- Sauvegarde dans le DataFrame local ---
        df_Results = pd.concat([
            df_Results,
            pd.DataFrame({
                'Methodology': [Methodology],
                'Model': ['Random Forest'],
                'Accuracy': [RF_test_score],
                'roc_value': [rf_roc_value],
                'threshold': [threshold]
            })
        ], ignore_index=True)

    return df_Results


# Cette fonction **`buildAndRunXGBoostModels`** permet de **construire, entra√Æner et √©valuer un mod√®le XGBoost** sur le dataset de transactions :  
# 
# - **Mod√®le XGBoost** :  
#   - `XGBClassifier` avec les param√®tres par d√©faut et `random_state=42` pour la reproductibilit√©.  
# - **Entra√Ænement** : le mod√®le est ajust√© sur les **donn√©es d'entra√Ænement** (`X_train`, `y_train`).  
# - **√âvaluation** :  
#   - Calcul de **l‚Äôaccuracy**, **matrice de confusion** et **classification report** sur l'ensemble de test.  
#   - Calcul des **probabilit√©s pr√©dites** pour la classe positive (fraude).  
#   - Calcul de la **valeur ROC-AUC** et du **seuil optimal** pour classifier une transaction comme frauduleuse.  
#   - Trac√© de la **courbe ROC** pour visualiser la performance du mod√®le.  
# - **Stockage des r√©sultats** : les performances du mod√®le (Accuracy, ROC-AUC, seuil) sont ajout√©es au **DataFrame `df_Results`** pour comparaison avec d'autres mod√®les.  
# 


def buildAndRunXGBoostModels(df_Results, Methodology, X_train, y_train, X_test, y_test):

    with mlflow.start_run(run_name=f"XGBoost_{Methodology}_{datetime.now().strftime('%H%M%S')}"):
        # --- Entra√Ænement du mod√®le ---
        XGBmodel = XGBClassifier(random_state=42, n_jobs=-1)
        XGBmodel.fit(X_train, y_train)
        y_pred = XGBmodel.predict(X_test)

        XGB_test_score = XGBmodel.score(X_test, y_test)

        # --- Affichage ---
        print(f'Model Accuracy: {XGB_test_score:.4f}')
        print("Confusion Matrix:\n", metrics.confusion_matrix(y_test, y_pred))
        print("Classification Report:\n", metrics.classification_report(y_test, y_pred))

        XGB_probs = XGBmodel.predict_proba(X_test)[:, 1]
        XGB_roc_value = metrics.roc_auc_score(y_test, XGB_probs)
        fpr, tpr, thresholds = metrics.roc_curve(y_test, XGB_probs)
        threshold = thresholds[np.argmax(tpr - fpr)]
        roc_auc = metrics.auc(fpr, tpr)

        print(f"XGB ROC AUC: {XGB_roc_value:.4f}")
        print(f"XGB Threshold: {threshold:.4f}")
        print("ROC for test dataset:", '{:.1%}'.format(roc_auc))

        plt.plot(fpr, tpr, label=f"Test, AUC={roc_auc:.4f}")
        plt.legend(loc=4)
        plt.show()

        # --- Log dans MLflow ---
        mlflow.log_param("Model", "XGBoost")
        mlflow.log_param("Methodology", Methodology)
        mlflow.log_metric("Accuracy", XGB_test_score)
        mlflow.log_metric("ROC_AUC", XGB_roc_value)
        mlflow.log_metric("Threshold", threshold)

        # Sauvegarde du mod√®le
        mlflow.xgboost.log_model(XGBmodel, artifact_path="model")

        # --- Sauvegarde dans le DataFrame local ---
        df_Results = pd.concat([
            df_Results,
            pd.DataFrame({
                'Methodology': [Methodology],
                'Model': ['XGBoost'],
                'Accuracy': [XGB_test_score],
                'roc_value': [XGB_roc_value],
                'threshold': [threshold]
            })
        ], ignore_index=True)

    return df_Results


# Cette fonction **`buildAndRunSVMModels`** permet de **construire, entra√Æner et √©valuer un mod√®le SVM (Support Vector Machine)** sur le dataset de transactions :  
# 
# - **Mod√®le SVM** :  
#   - `SVC` avec le **kernel sigmoid** pour capturer des relations non lin√©aires, et `random_state=42` pour la reproductibilit√©.  
# - **Entra√Ænement** : le mod√®le est ajust√© sur les **donn√©es d'entra√Ænement** (`X_train`, `y_train`).  
# - **√âvaluation** :  
#   - Calcul de **l‚Äôaccuracy**, **matrice de confusion** et **classification report** sur l'ensemble de test.  
#   - Pour obtenir des probabilit√©s, un second SVM est entra√Æn√© avec `probability=True`.  
#   - Calcul de la **valeur ROC-AUC** et du **seuil optimal** pour classifier une transaction comme frauduleuse.  
#   - Trac√© de la **courbe ROC** pour visualiser la performance du mod√®le.  
# - **Stockage des r√©sultats** : les performances du mod√®le (Accuracy, ROC-AUC, seuil) sont ajout√©es au **DataFrame `df_Results`** pour comparaison avec d'autres mod√®les.  
# 


def buildAndRunSVMModels(df_Results, Methodology, X_train, y_train, X_test, y_test):

    with mlflow.start_run(run_name=f"SVM_{Methodology}_{datetime.now().strftime('%H%M%S')}"):
        # --- Entra√Ænement du mod√®le ---
        clf = SVC(kernel='sigmoid', probability=True, random_state=42)
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        SVM_score = metrics.accuracy_score(y_test, y_pred)

        # --- Affichage ---
        print(f'Accuracy score: {SVM_score:.4f}')
        print("Confusion Matrix:\n", metrics.confusion_matrix(y_test, y_pred))
        print("Classification Report:\n", metrics.classification_report(y_test, y_pred))

        svm_probs = clf.predict_proba(X_test)[:, 1]
        SVM_roc_value = metrics.roc_auc_score(y_test, svm_probs)
        fpr, tpr, thresholds = metrics.roc_curve(y_test, svm_probs)
        threshold = thresholds[np.argmax(tpr - fpr)]
        roc_auc = metrics.auc(fpr, tpr)

        print(f"SVM ROC AUC: {SVM_roc_value:.4f}")
        print(f"SVM Threshold: {threshold:.4f}")
        print("ROC for test dataset:", '{:.1%}'.format(roc_auc))

        plt.plot(fpr, tpr, label=f"Test, AUC={roc_auc:.4f}")
        plt.legend(loc=4)
        plt.show()

        # --- Log dans MLflow ---
        mlflow.log_param("Model", "SVM")
        mlflow.log_param("Methodology", Methodology)
        mlflow.log_param("Kernel", "sigmoid")
        mlflow.log_metric("Accuracy", SVM_score)
        mlflow.log_metric("ROC_AUC", SVM_roc_value)
        mlflow.log_metric("Threshold", threshold)

        # Sauvegarde du mod√®le
        mlflow.sklearn.log_model(clf, artifact_path="model")

        # --- Sauvegarde dans le DataFrame local ---
        df_Results = pd.concat([
            df_Results,
            pd.DataFrame({
                'Methodology': [Methodology],
                'Model': ['SVM'],
                'Accuracy': [SVM_score],
                'roc_value': [SVM_roc_value],
                'threshold': [threshold]
            })
        ], ignore_index=True)

    return df_Results


# Ce bloc utilise la **validation crois√©e r√©p√©t√©e (Repeated K-Fold)** pour √©valuer la robustesse des mod√®les :  
# 
# - `RepeatedKFold` divise les donn√©es en **5 plis** et r√©p√®te la proc√©dure **10 fois**.  
# - Pour chaque it√©ration :  
#   - `train_index` et `test_index` contiennent les indices des **donn√©es d'entra√Ænement** et **de test**.  
#   - `X_train_cv` et `X_test_cv` contiennent les **features** pour l'entra√Ænement et le test.  
#   - `y_train_cv` et `y_test_cv` contiennent les **cibles** correspondantes.  
# - L‚Äôimpression des indices montre quelles observations sont utilis√©es pour l‚Äôentra√Ænement et le test √† chaque pli.  
# 
# Cette technique permet de **r√©duire la variance des estimations de performance** et d‚Äô**√©valuer les mod√®les de mani√®re plus fiable** sur des donn√©es d√©s√©quilibr√©es.
# 


rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)
for train_index, test_index in rkf.split(X,y):
    print("TRAIN:", train_index, "TEST", test_index)
    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]
    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]

# -
# 
# ### 1Ô∏è‚É£ **Logistic Regression**
# 
# * La r√©gression logistique est ex√©cut√©e avec deux r√©gularisations : **L1** et **L2**.
# * Le code calcule :
# 
#   * **Accuracy**
#   * **Confusion Matrix**
#   * **Classification Report**
#   * **ROC-AUC et seuil optimal**
# * R√©sultat cl√© :
# 
#   * L1 a un **roc\_value = 0.88**, L2 a **roc\_value = 0.57**, donc L1 est beaucoup meilleur pour ce jeu de donn√©es.
# * Temps d‚Äôex√©cution : \~126 secondes.
# 
# ---
# 
# ### 2Ô∏è‚É£ **KNN**
# 
# * K-Nearest Neighbors avec `n_neighbors=5`.
# * Mesure de performance : Accuracy, ROC-AUC, seuil optimal.
# * R√©sultats :
# 
#   * Accuracy ‚âà 0.999
#   * ROC ‚âà 0.876
# * Temps d‚Äôex√©cution : \~239 secondes.
# 
# ---
# 
# ### 3Ô∏è‚É£ **Decision Tree**
# 
# * Deux crit√®res test√©s : **gini** et **entropy**.
# * Mesure de performance : Accuracy, ROC-AUC, seuil optimal.
# * R√©sultats :
# 
#   * gini : ROC ‚âà 0.881
#   * entropy : ROC ‚âà 0.876
# * Temps d‚Äôex√©cution : \~54 secondes.
# 
# ---
# 
# ### 4Ô∏è‚É£ **Random Forest**
# 
# * Mod√®le avec 100 arbres, `max_features='sqrt'`.
# * R√©sultats :
# 
#   * Accuracy ‚âà 0.9995
#   * ROC ‚âà 0.937
# * Temps d‚Äôex√©cution : \~352 secondes.
# 
# ---
# 
# ### 5Ô∏è‚É£ **XGBoost**
# 
# * Mod√®le boosting avec `XGBClassifier`.
# * R√©sultats :
# 
#   * Accuracy ‚âà 0.9995
#   * ROC ‚âà 0.978 ‚Üí Meilleur mod√®le ROC de tous.
# * Temps d‚Äôex√©cution : \~4.6 secondes (tr√®s rapide pour XGBoost).
# 
# ---
# 
# ### ‚úÖ **R√©sum√© g√©n√©ral**
# 
# * **Meilleur ROC** : XGBoost (97.8%)
# * **Plus rapide** : XGBoost (\~4s), malgr√© Random Forest plus lent (\~352s).
# * **Seuil optimal** : calcul√© via `tpr-fpr` pour tous les mod√®les afin de maximiser le ROC.
# * Ce bloc montre comment **ex√©cuter plusieurs mod√®les, mesurer per, ROC et temps** pour que ce soit plus lisible.
# 
# Veux‚Äëtu que je fasse √ßa‚ÄØ?
# 


print("Logistic Regression with 11 and 12 Regularisation")
start_time = time.time()
df_Results = buildAndRunLogisticModels(df_Results, "RepeatedKFold Cross Validation", X_train_cv, y_train_cv, X_test_cv, y_test_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("KNN Model")
start_time = time.time()
df_Results = buildAndRunKNNModels(df_Results, "RepeatedKFold Cross Validation", X_train_cv, y_train_cv, X_test_cv, y_test_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("Decision Tree Models with 'gini' and 'entropy' criteria")
start_time = time.time()
df_Results = buildAndRunTreeModels(df_Results, "RepeatedKFold Cross Validation", X_train_cv, y_train_cv, X_test_cv, y_test_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("Random Forest Model")
start_time = time.time()
df_Results = buildAndRunRandomForestModels(df_Results, "RepeatedKFold Cross Validation", X_train_cv, y_train_cv, X_test_cv, y_test_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("XGBoost Model")
start_time = time.time()
df_Results = buildAndRunXGBoostModels(df_Results, "RepeatedKFold Cross Validation", X_train_cv, y_train_cv, X_test_cv, y_test_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )


df_Results

# Ce bloc utilise la **validation crois√©e stratifi√©e (Stratified K-Fold)** pour maintenir la **proportion des classes** dans chaque pli :  
# 
# - `StratifiedKFold` divise les donn√©es en **5 plis**, en conservant la m√™me r√©partition des classes dans les ensembles d'entra√Ænement et de test.  
# - Pour chaque pli :  
#   - `train_index` et `test_index` contiennent les indices des **donn√©es d'entra√Ænement** et **de test**.  
#   - `X_train_SKF_cv` et `X_test_SKF_cv` contiennent les **features** correspondantes.  
#   - `y_train_SKF_cv` et `y_test_SKF_cv` contiennent les **cibles** correspondantes.  
# - L‚Äôimpression des indices montre quelles observations sont utilis√©es pour l‚Äôentra√Ænement et le test √† chaque pli.  
# 
# Cette approche est particuli√®rement utile pour les datasets **d√©s√©quilibr√©s**, comme celui de la d√©tection de fraude, afin de **pr√©server la proportion des fraudes** dans chaque pli.
# 


skf = StratifiedKFold(n_splits=5, random_state=None)
for train_index, test_index in skf.split(X,y):
    print("TRAIN:", train_index, "TEST", test_index)
    X_train_SKF_cv, X_test_SKF_cv = X.iloc[train_index], X.iloc[test_index]
    y_train_SKF_cv, y_test_SKF_cv = y.iloc[train_index], y.iloc[test_index]

# 
# #### **Logistic Regression (L1 & L2 Regularisation)**
# 
# * L1 (lasso) : meilleur ROC AUC = 0.889 ‚Üí seuil optimal ‚âà 0.021.
# * L2 (ridge) : ROC AUC faible = 0.611 ‚Üí seuil ‚âà 0.499.
# * L1 g√®re mieux la d√©tection de la classe minoritaire.
# * Accuracy global tr√®s √©lev√©e (\~0.9987 pour L1).
# 
# #### **KNN**
# 
# * Accuracy = 0.9992
# * ROC AUC = 0.806, seuil = 0.2
# * Bonne performance globale, mais recall de la classe minoritaire plus faible que Logistic L1.
# 
# #### **Decision Tree**
# 
# * Crit√®re Gini : Accuracy = 0.9988, ROC AUC = 0.826
# * Crit√®re Entropy : Accuracy = 0.9990, ROC AUC = 0.821
# * Bonnes performances, l√©g√®rement inf√©rieures aux mod√®les d‚Äôensemble.
# 
# #### **Random Forest**
# 
# * Accuracy = 0.9994
# * ROC AUC = 0.946 ‚Üí seuil tr√®s faible = 0.01
# * Tr√®s bon √©quilibre entre pr√©cision et rappel pour la classe minoritaire.
# 
# #### **XGBoost**
# 
# * Accuracy = 0.9994
# * ROC AUC = 0.972 ‚Üí seuil tr√®s faible ‚âà 3.77e-5
# * Meilleure performance globale, surtout pour la d√©tection des cas minoritaires.
# * Temps d‚Äôentra√Ænement tr√®s rapide par rapport aux autres mod√®les d‚Äôensemble.


print("Logistic Regression with 11 and 12 Regularisation")
start_time = time.time()
df_Results = buildAndRunLogisticModels(df_Results, "StratifiedKFold Cross Validation", X_train_SKF_cv, y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("KNN Model")
start_time = time.time()
df_Results = buildAndRunKNNModels(df_Results, "StratifiedKFold Cross Validation", X_train_SKF_cv, y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("Decision Tree Models with 'gini' and 'entropy' criteria")
start_time = time.time()
df_Results = buildAndRunTreeModels(df_Results, "StratifiedKFold Cross Validation", X_train_SKF_cv, y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("Random Forest Model")
start_time = time.time()
df_Results = buildAndRunRandomForestModels(df_Results, "StratifiedKFold Cross Validation", X_train_SKF_cv, y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

print("XGBoost Model")
start_time = time.time()
df_Results = buildAndRunXGBoostModels(df_Results, "StratifiedKFold Cross Validation", X_train_SKF_cv, y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)
print("Time Taken by Model: --- %s seconds ---"% ( time.time() - start_time))
print('-'*60 )

df_Results 


param_test = {
    'max_depth':range(3,10,2),
    'min_child_weight':range(1,6,2),
    'n_estimators':range(60,130,150),
    'learning_rate':[0.05,0.1,0.125,0.15,0.2],
    'gamma':[i/10.0 for i in range(0,5)],
    'subsample':[i/10.0 for i in range(7,10)],
    'colsample_bytree':[i/10.0 for i in range(7,10)]
}

gsearch1 = RandomizedSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                                                        colsample_bynode=1,max_delta_step=0,
                                                        missing=None, n_jobs=-1,
                                                        nthread=None, objective='binary:logistic', random_state=42,
                                                        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
                                                        silent=None, verbosity=1),
                                                        param_distributions = param_test, n_iter=5, scoring='roc_auc', n_jobs=-1, cv=5)
gsearch1.fit(X_train_cv, y_train_cv)
gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_

# Enfin, nous d√©finissons et entra√Ænons le **classifieur XGBoost final** avec les meilleurs hyperparam√®tres trouv√©s lors de la recherche al√©atoire.  
# 
# - **Configuration du mod√®le** :  
#   - `max_depth=9` et `min_child_weight=1` pour g√©rer la complexit√© des arbres.  
#   - `n_estimators=60` et `learning_rate=0.2` pour contr√¥ler le nombre d‚Äôarbres et la vitesse d‚Äôapprentissage.  
#   - `subsample=0.7` et `colsample_bytree=0.9` pour r√©duire le surapprentissage.  
#   - `gamma=0.3` pour r√©gulariser et √©viter les branches inutiles.  
#   - `objective='binary:logistic'` car il s‚Äôagit d‚Äôun probl√®me de classification binaire (fraude ou non).  
# 
# - **Entra√Ænement** : le mod√®le est entra√Æn√© sur les donn√©es **oversampl√©es (X_over, y_over)** pour mieux g√©rer le d√©s√©quilibre des classes.  
# - **√âvaluation** :  
#   - `XGB_test_score` : pr√©cision globale sur l‚Äôensemble de test.  
#   - `XGB_probs` : probabilit√©s pr√©dites pour la classe positive (fraude).  
#   - `XGB_roc_value` : score ROC-AUC, mesurant la capacit√© du mod√®le √† s√©parer les classes.  
#   - `optimal_threshold` : seuil optimal calcul√© √† partir de la courbe ROC pour maximiser la sensibilit√© et la sp√©cificit√©.
#     
#   
#   - **Model accuracy** : 0.9994  
#   - **XGB ROC-AUC** 0.9807  
#   - **Seuil optimal** : 0.0005  
# 
# Ce mod√®le XGBoost montre donc une **excellente capacit√© √† d√©tecter les transactions frauduleuses** dans notre dataset.
# 


# Define the XGBoost classifier with updated parameters
clf = XGBClassifier(
    base_score=0.5,
    booster='gbtree',
    colsample_bylevel=1,
    colsample_bynode=1,
    colsample_bytree=0.9,
    gamma=0.3,
    learning_rate=0.2,
    max_delta_step=0,
    max_depth=9,
    min_child_weight=1,
    n_estimators=60,
    n_jobs=1,
    objective='binary:logistic',
    random_state=42,
    reg_alpha=0,
    reg_lambda=1,
    scale_pos_weight=1,
    subsample=0.7,
    verbosity=1
)

# Train the classifier
clf.fit(X_train_cv, y_train_cv)

# Evaluate accuracy on the test set
XGB_test_score = clf.score(X_test_cv, y_test_cv)
print(f"Model accuracy: {XGB_test_score:.4f}")

# Get predicted probabilities for the positive class
XGB_probs = clf.predict_proba(X_test_cv)[:, 1]

# Calculate the ROC-AUC score
XGB_roc_value = metrics.roc_auc_score(y_test_cv, XGB_probs)
print(f"XGB ROC-AUC value: {XGB_roc_value:.4f}")

# Calculate ROC curve and find the optimal threshold
fpr, tpr, thresholds = metrics.roc_curve(y_test_cv, XGB_probs)
optimal_threshold = thresholds[np.argmax(tpr - fpr)]
print(f"XGB optimal threshold: {optimal_threshold:.4f}")

# D√©marrer une nouvelle exp√©rience MLflow
with mlflow.start_run(run_name=f"XGBOOST_Final_Result_{datetime.now().strftime('%H%M%S')}"):
    # Enregistrer les param√®tres du mod√®le
    mlflow.log_params(clf.get_params())

    # Enregistrer le mod√®le XGBoost
    mlflow.xgboost.log_model(clf, "model")

    # Enregistrer les m√©triques
    mlflow.log_metric("accuracy", XGB_test_score)
    mlflow.log_metric("roc_auc", XGB_roc_value)
    mlflow.log_metric("optimal_threshold", optimal_threshold)


df_Results.to_csv("results.csv", index=False)
mlflow.log_artifact("results.csv")

import mlflow.pyfunc

best_model_uri = "runs:/cec76408088449a09313e4f7f1e1335c/model"
model_details = mlflow.register_model(best_model_uri, "CreditCardFraudModel")


from mlflow.tracking import MlflowClient

client = MlflowClient()
client.transition_model_version_stage(
    name="CreditCardFraudModel",
    version=model_details.version,
    stage="Production"
)